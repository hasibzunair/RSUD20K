{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvUXqXixHAx8"
      },
      "source": [
        "## Auto label data using foundation models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byokHef__Jfd",
        "outputId": "5c524a38-bafa-4204-9530-0d86e0629907"
      },
      "outputs": [],
      "source": [
        "# # Do once\n",
        "# %pip install -q \\\n",
        "#  autodistill \\\n",
        "#  autodistill-detic \\\n",
        "#  autodistill-grounding-dino \\\n",
        "#  supervision==0.9.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt0DJTpvUIm5"
      },
      "source": [
        "### Display image sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hKxoZuw2Mze",
        "outputId": "d1f80d49-db4a-4e02-a3c1-2c4a90138c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image count: 3985\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "\n",
        "HOME = os.getcwd()\n",
        "IMAGE_DIR_PATH = f\"{HOME}/images\"\n",
        "\n",
        "image_paths = sv.list_files_with_extensions(\n",
        "    directory=IMAGE_DIR_PATH,\n",
        "    extensions=[\"png\", \"jpg\"])\n",
        "\n",
        "print('image count:', len(image_paths))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3mlA1Xw2ZdV"
      },
      "source": [
        "Plot sample of our image dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eaBaWn-DUaec"
      },
      "outputs": [],
      "source": [
        "SAMPLE_SIZE = 16\n",
        "SAMPLE_GRID_SIZE = (4, 4)\n",
        "SAMPLE_PLOT_SIZE = (16, 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QQEvtmvnN_ih",
        "outputId": "e94079d3-4fd4-42ec-db1f-841d74cc6771"
      },
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# import supervision as sv\n",
        "\n",
        "# titles = [\n",
        "#     image_path.stem\n",
        "#     for image_path\n",
        "#     in image_paths[:SAMPLE_SIZE]]\n",
        "# images = [\n",
        "#     cv2.imread(str(image_path))\n",
        "#     for image_path\n",
        "#     in image_paths[:SAMPLE_SIZE]]\n",
        "\n",
        "# sv.plot_images_grid(images=images, titles=titles, grid_size=SAMPLE_GRID_SIZE, size=SAMPLE_PLOT_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCMpPz3wVb_M"
      },
      "source": [
        "### Autolabel images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIVuc89kVp2w"
      },
      "source": [
        "Here we define ontology.\n",
        "\n",
        "**Ontology** - an Ontology defines how your Base Model is prompted, what your Dataset will describe, and what your Target Model will predict. A simple Ontology is the CaptionOntology which prompts a Base Model with text captions and maps them to class names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "24qFSVyhUV8C"
      },
      "outputs": [],
      "source": [
        "from autodistill.detection import CaptionOntology\n",
        "\n",
        "# left -> prompt sent to model (should be descriptive)\n",
        "# right -> class label we want the model to predict\n",
        "\n",
        "# description from ChatGPT\n",
        "# prompt: describe what a {class name} looks like in 15-20 words\n",
        "\n",
        "ontology=CaptionOntology({\n",
        "    \"a person is a living being with a complex physical form including a head torso limbs and varied appearance based on ethnicity and individual traits\": \"person\",\n",
        "    \"a rickshaw is a human powered or motorized vehicle with a simple frame seating and often two or three wheels\" : \"rickshaw\",\n",
        "    \"a rickshaw van is a motorized three wheeled vehicle with an enclosed cabin for passengers or goods and typically a driver upfront\": \"rickshaw van\",\n",
        "    \"an auto rickshaw is a compact three wheeled motorized vehicle with a cabin for passengers a driver upfront and a rear engine\": \"auto rickshaw\",\n",
        "    \"a truck is a large motorized vehicle with a drivers cabin cargo area wheels and often a distinct front grille\": \"truck\",\n",
        "    \"a pickup truck is a smaller motorized vehicle with a drivers cabin and an open cargo bed in the rear\": \"pickup truck\",\n",
        "    \"a private car is a four wheeled motor vehicle designed for personal transportation typically with seating for passengers and an enclosed cabin\": \"private car\",\n",
        "    \"a motorcycle is a two wheeled motor vehicle with a seat for a rider and often a pillion seat for a passenger\": \"motorcycle\",\n",
        "    \"a bicycle is a human powered vehicle with two wheels pedals a frame handlebars and a seat for a rider\": \"bicycle\",\n",
        "    \"a bus is a large motorized vehicle with a passenger cabin typically featuring multiple seats windows and a distinctive elongated shape\": \"bus\",\n",
        "    \"a micro bus is a smaller motorized vehicle similar to a standard bus but more compact with seating for fewer passengers\": \"micro bus\",\n",
        "    \"a covered van is a motorized vehicle with a closed cargo area often used for transporting goods and may have a drivers cabin upfront\": \"covered van\",\n",
        "    \"a human hauler is a motorized vehicle designed for transporting passengers similar to an auto rickshaw or tuktuk with a cabin and driver upfront\": \"human hauler\"\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXyoVtz_WGYq"
      },
      "source": [
        "### Initiate base model and autolabel\n",
        "\n",
        "**Base Model** - A Base Model is a large foundation model that knows a lot about a lot. We use a Base Model (along with unlabeled input data and an Ontology) to create a Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jmJS9aJV5VW",
        "outputId": "76923c05-ab61-49d9-acb8-f32bfd6db464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a person is a living being with a complex physical form including a head torso limbs and varied appearance based on ethnicity and individual traits, a rickshaw is a human powered or motorized vehicle with a simple frame seating and often two or three wheels, a rickshaw van is a motorized three wheeled vehicle with an enclosed cabin for passengers or goods and typically a driver upfront, an auto rickshaw is a compact three wheeled motorized vehicle with a cabin for passengers a driver upfront and a rear engine, a truck is a large motorized vehicle with a drivers cabin cargo area wheels and often a distinct front grille, a pickup truck is a smaller motorized vehicle with a drivers cabin and an open cargo bed in the rear, a private car is a four wheeled motor vehicle designed for personal transportation typically with seating for passengers and an enclosed cabin, a motorcycle is a two wheeled motor vehicle with a seat for a rider and often a pillion seat for a passenger, a bicycle is a human powered vehicle with two wheels pedals a frame handlebars and a seat for a rider, a bus is a large motorized vehicle with a passenger cabin typically featuring multiple seats windows and a distinctive elongated shape, a micro bus is a smaller motorized vehicle similar to a standard bus but more compact with seating for fewer passengers, a covered van is a motorized vehicle with a closed cargo area often used for transporting goods and may have a drivers cabin upfront, a human hauler is a motorized vehicle designed for transporting passengers similar to an auto rickshaw or tuktuk with a cabin and driver upfront\n",
            "Loading pretrained CLIP\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/hasib/anaconda3/envs/bdstreets/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[11/02 14:54:35 fvcore.common.checkpoint]: \u001b[0m[Checkpointer] Loading from models/Detic_LCOCOI21k_CLIP_SwinB_896b32_4x_ft4x_max-size.pth ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Labeling /home/hasib/bdstreets-dataset/auto_labeler/images/train12933.jpg:   1%|          | 24/3985 [00:28<1:50:51,  1.68s/it]"
          ]
        }
      ],
      "source": [
        "# DETIC\n",
        "from autodistill_detic import DETIC\n",
        "DATASET_DIR_PATH = f\"{HOME}/bdss_detic\"\n",
        "base_model = DETIC(ontology=ontology)\n",
        "dataset = base_model.label(\n",
        "    input_folder=IMAGE_DIR_PATH,\n",
        "    extension=\".jpg\",\n",
        "    output_folder=DATASET_DIR_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM9zH9BGZGp0"
      },
      "source": [
        "### Display dataset sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsTHheE-bMbY"
      },
      "outputs": [],
      "source": [
        "# ANNOTATIONS_DIRECTORY_PATH = f\"{HOME}/bdss_detic/images/train\"\n",
        "# IMAGES_DIRECTORY_PATH = f\"{HOME}/bdss_detic/labels/train\"\n",
        "# DATA_YAML_PATH = f\"{HOME}/bdss_detic/data.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uhgy9rbdYSCZ",
        "outputId": "b89226c5-918b-46f8-cc2d-16fc2967332b"
      },
      "outputs": [],
      "source": [
        "# import supervision as sv\n",
        "\n",
        "# dataset = sv.DetectionDataset.from_yolo(\n",
        "#     images_directory_path=IMAGES_DIRECTORY_PATH,\n",
        "#     annotations_directory_path=ANNOTATIONS_DIRECTORY_PATH,\n",
        "#     data_yaml_path=DATA_YAML_PATH)\n",
        "\n",
        "# len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "krNCDdDZcm7H",
        "outputId": "a00da65c-1b6d-444b-a358-e1c8ea1cd39f"
      },
      "outputs": [],
      "source": [
        "# import supervision as sv\n",
        "\n",
        "# image_names = list(dataset.images.keys())[:SAMPLE_SIZE]\n",
        "# box_annotator = sv.BoxAnnotator()\n",
        "\n",
        "# images = []\n",
        "# for image_name in image_names:\n",
        "#     image = dataset.images[image_name]\n",
        "#     annotations = dataset.annotations[image_name]\n",
        "#     labels = [\n",
        "#         dataset.classes[class_id]\n",
        "#         for class_id\n",
        "#         in annotations.class_id]\n",
        "#     annotates_image = box_annotator.annotate(\n",
        "#         scene=image.copy(),\n",
        "#         detections=annotations,\n",
        "#         labels=labels)\n",
        "#     images.append(annotates_image)\n",
        "\n",
        "# sv.plot_images_grid(\n",
        "#     images=images,\n",
        "#     titles=image_names,\n",
        "#     grid_size=SAMPLE_GRID_SIZE,\n",
        "#     size=SAMPLE_PLOT_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwdCGgRwfxnk"
      },
      "outputs": [],
      "source": [
        "# if not os.path.exists(\"./detic_results\"):\n",
        "#   os.makedirs(\"./detic_results\")\n",
        "\n",
        "# for img, name in zip(images, image_names):\n",
        "#   cv2.imwrite(f\"./detic_results/{name}\", img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "w-pt77lmEez5",
        "outputId": "3306de2f-ce02-4c34-a1f1-abe5783a7f07"
      },
      "outputs": [],
      "source": [
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# !zip -r ./detic_results.zip ./detic_results\n",
        "# from google.colab import files\n",
        "# files.download(\"./detic_results.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPcy33O_jAZk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
